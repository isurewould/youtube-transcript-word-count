{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from pytube import YouTube\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ian-\n",
      "[nltk_data]     coding/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download stop words from nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your video URL or video ID\n",
    "video_url = \"https://www.youtube.com/watch?v=kCc8FmEb1nY&t\"\n",
    "video = YouTube(video_url)\n",
    "video_id = video.video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch transcript\n",
    "transcript = YouTubeTranscriptApi.get_transcript(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           Count     \n",
      "=========================\n",
      "tokens         64        \n",
      "model          61        \n",
      "size           60        \n",
      "transformer    55        \n",
      "data           54        \n",
      "actually       53        \n",
      "token          44        \n",
      "block          44        \n",
      "b              44        \n",
      "sort           43        \n",
      "batch          43        \n",
      "layer          43        \n",
      "would          42        \n",
      "number         42        \n",
      "okay           41        \n",
      "characters     41        \n",
      "also           41        \n",
      "self           41        \n",
      "first          40        \n",
      "training       38        \n"
     ]
    }
   ],
   "source": [
    "# Flatten transcript text\n",
    "all_text = \" \".join([segment['text'] for segment in transcript])\n",
    "\n",
    "# Tokenize words\n",
    "words = re.findall(r'\\b\\w+\\b', all_text.lower())\n",
    "\n",
    "# Extend nltk's list with your own stop words\n",
    "custom_stop_words = {'the', 'and', 'to', 'we', 'so', 'a', 'is', 'of', 'that', 'this', 'it', 'in', 's', 'i', 'here', 'now', 'just', 're', 'you', 'then',\n",
    "                     'going', 'like', 'one', 'uh', 'um', 'basically', 'want', 'let', 'get', 'attention', 'see', 'way'}\n",
    "stop_words = set(stopwords.words('english')).union(custom_stop_words)\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Count occurrences\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# Get top 20 most common words\n",
    "top_20_words = word_counts.most_common(20)\n",
    "\n",
    "# Output as a table\n",
    "print(f'{\"Word\":<15}{\"Count\":<10}')\n",
    "print(\"=\" * 25)\n",
    "for word, count in top_20_words:\n",
    "    print(f\"{word:<15}{count:<10}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
